{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec (Tensorflow-2.0 And Keras Implementation).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sehgalsakshi/Word2Vec-Skip-Gram-Neural-Network-Implementation-from-Scratch/blob/main/Word2Vec_(Tensorflow_2_0_And_Keras_Implementation).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkiPZ4JdHgpH"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjkU27evHlpM"
      },
      "source": [
        "### Word2Vec is an unsupervised technique for modelling word embeddings in a way that words similar to each other are placed on similar positions in word vector.\n",
        "<b>Word Vector?</b> What is that?\n",
        "<ul>\n",
        "<li>Before word vector, we had (still have!) plain and simple word embeddings where each word is represented by a <b>sparse matrix</b> representation, represented by 1 (or the count of specific word) corresponding to the document it is present in.</li>\n",
        "<li>Word Embedding was a crude way of representing words but was somewhat improved using approaches like <b>TF-IDF</b> which emphasizes on repetition of word within document but rarity within corpus to show the importance. </li>\n",
        "<li>Word embeddings just told us about the presence or absence of word but what gives meaning to a sentence is presence of a word with respect to other words</li>\n",
        "<li>This is exactly what word vector does! Word vector is just a vector that explains a meaning of the word. </li>\n",
        "<li>It does so by having different features to represent a word. These features are the context words that may or may not be surrounding the given word.</li>\n",
        "<li><b>a word vector is a row of real valued numbers (as opposed to dummy numbers) where each point captures a dimension of the wordâ€™s meaning and where semantically similar words have similar vectors.</b></li>\n",
        "<li>Since it's the mathematical representation of <b>word meanings</b>, hence there should be a way to extract mathematical equations for word meanings</li>\n",
        "<li> Using word vectors we can get equations like <i>King - Man + Woman = Queen </i></li> \n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_HN6amnK5Yt"
      },
      "source": [
        "# The Corpus\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ekF88NEHOKv"
      },
      "source": [
        "corpus = ['king is a strong man', \n",
        "          'queen is a wise woman', \n",
        "          'boy is a young man',\n",
        "          'girl is a young woman',\n",
        "          'prince is a young king',\n",
        "          'princess is a young queen',\n",
        "          'man is strong', \n",
        "          'woman is pretty',\n",
        "          'prince is a boy will be king',\n",
        "          'princess is a girl will be queen']"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkWRXofRMB5t"
      },
      "source": [
        "def remove_stop_words(corpus):\n",
        "    stop_words = ['is', 'a', 'will', 'be']\n",
        "    results = []\n",
        "    for text in corpus:\n",
        "        tmp = text.split(' ')\n",
        "        for stop_word in stop_words:\n",
        "            if stop_word in tmp:\n",
        "                tmp.remove(stop_word)\n",
        "        results.append(\" \".join(tmp))\n",
        "    \n",
        "    return results"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDpTb91QMEZh"
      },
      "source": [
        "corpus = remove_stop_words(corpus)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQzrq2UkMKXm"
      },
      "source": [
        "words = []\n",
        "for text in corpus:\n",
        "    for word in text.split(' '):\n",
        "        words.append(word)\n",
        "\n",
        "words = set(words)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRhYQ5MhMMrq"
      },
      "source": [
        "here we have word set by which we will have word vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjrAMd96MQEk",
        "outputId": "48c32710-5b54-4ac5-bfd4-d3854bc9522a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "words"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boy',\n",
              " 'girl',\n",
              " 'king',\n",
              " 'man',\n",
              " 'pretty',\n",
              " 'prince',\n",
              " 'princess',\n",
              " 'queen',\n",
              " 'strong',\n",
              " 'wise',\n",
              " 'woman',\n",
              " 'young'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBy7II7vMVKt"
      },
      "source": [
        "# Data Generation\n",
        "we will generate label for each word using skip gram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScFGuiM4MUaH"
      },
      "source": [
        "word2int = {}\n",
        "\n",
        "for i,word in enumerate(words):\n",
        "    #saving word index to get word embedding\n",
        "    word2int[word] = i\n",
        "\n",
        "sentences = []\n",
        "for sentence in corpus:\n",
        "    #Each sentence is represented as a list of words\n",
        "    sentences.append(sentence.split())\n",
        "    \n",
        "WINDOW_SIZE = 2\n",
        "\n",
        "data = []\n",
        "for sentence in sentences:\n",
        "    for idx, word in enumerate(sentence):\n",
        "        for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentence)) + 1] : \n",
        "            if neighbor != word:\n",
        "                data.append([word, neighbor])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe0eMlZpMdjA"
      },
      "source": [
        "Window size of 2 here says meaning of word can be dedices by looking at word with 2 neighboring words in both direction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSKprgWwMriD",
        "outputId": "d6dcdf7f-0276-44af-eeca-0ac8ff92ec26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['king', 'strong'],\n",
              " ['king', 'man'],\n",
              " ['strong', 'king'],\n",
              " ['strong', 'man'],\n",
              " ['man', 'king'],\n",
              " ['man', 'strong'],\n",
              " ['queen', 'wise'],\n",
              " ['queen', 'woman'],\n",
              " ['wise', 'queen'],\n",
              " ['wise', 'woman'],\n",
              " ['woman', 'queen'],\n",
              " ['woman', 'wise'],\n",
              " ['boy', 'young'],\n",
              " ['boy', 'man'],\n",
              " ['young', 'boy'],\n",
              " ['young', 'man'],\n",
              " ['man', 'boy'],\n",
              " ['man', 'young'],\n",
              " ['girl', 'young'],\n",
              " ['girl', 'woman'],\n",
              " ['young', 'girl'],\n",
              " ['young', 'woman'],\n",
              " ['woman', 'girl'],\n",
              " ['woman', 'young'],\n",
              " ['prince', 'young'],\n",
              " ['prince', 'king'],\n",
              " ['young', 'prince'],\n",
              " ['young', 'king'],\n",
              " ['king', 'prince'],\n",
              " ['king', 'young'],\n",
              " ['princess', 'young'],\n",
              " ['princess', 'queen'],\n",
              " ['young', 'princess'],\n",
              " ['young', 'queen'],\n",
              " ['queen', 'princess'],\n",
              " ['queen', 'young'],\n",
              " ['man', 'strong'],\n",
              " ['strong', 'man'],\n",
              " ['woman', 'pretty'],\n",
              " ['pretty', 'woman'],\n",
              " ['prince', 'boy'],\n",
              " ['prince', 'king'],\n",
              " ['boy', 'prince'],\n",
              " ['boy', 'king'],\n",
              " ['king', 'prince'],\n",
              " ['king', 'boy'],\n",
              " ['princess', 'girl'],\n",
              " ['princess', 'queen'],\n",
              " ['girl', 'princess'],\n",
              " ['girl', 'queen'],\n",
              " ['queen', 'princess'],\n",
              " ['queen', 'girl']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOG2WYflNW3M",
        "outputId": "d3c138d4-26a4-4e23-b13d-34a20ccf01f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import pandas as pd\n",
        "for text in corpus:\n",
        "    #corpus is a list of documents, we're just printing individual documents(sentences)\n",
        "    print(text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "king strong man\n",
            "queen wise woman\n",
            "boy young man\n",
            "girl young woman\n",
            "prince young king\n",
            "princess young queen\n",
            "man strong\n",
            "woman pretty\n",
            "prince boy king\n",
            "princess girl queen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLcthux_OCRq"
      },
      "source": [
        "df = pd.DataFrame(data, columns = ['input', 'label'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS4W37rJOJqp",
        "outputId": "674f5e8d-86a3-4122-a60a-2e4b7a619b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>king</td>\n",
              "      <td>strong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>king</td>\n",
              "      <td>man</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>strong</td>\n",
              "      <td>king</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>strong</td>\n",
              "      <td>man</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>man</td>\n",
              "      <td>king</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    input   label\n",
              "0    king  strong\n",
              "1    king     man\n",
              "2  strong    king\n",
              "3  strong     man\n",
              "4     man    king"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz26sknOOSea"
      },
      "source": [
        "Data's input is the word and labels are the context words around input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7bFfUL6Ohwj",
        "outputId": "82059224-9b15-4465-c707-9bdcd136276f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKHIXxlCOk4j",
        "outputId": "8165a3a1-a254-488a-fa66-91aea4838ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "\n",
        "word2int"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boy': 4,\n",
              " 'girl': 7,\n",
              " 'king': 1,\n",
              " 'man': 0,\n",
              " 'pretty': 3,\n",
              " 'prince': 11,\n",
              " 'princess': 8,\n",
              " 'queen': 9,\n",
              " 'strong': 5,\n",
              " 'wise': 10,\n",
              " 'woman': 6,\n",
              " 'young': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVlqrGYiOsO7"
      },
      "source": [
        "# Define Tensorflow Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5aGTSyKOu4h"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sMyleWZOzN4"
      },
      "source": [
        "ONE_HOT_DIM = len(words)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw_knBXqO136"
      },
      "source": [
        "# function to convert numbers to one hot vectors\n",
        "def to_one_hot_encoding(data_point_index):\n",
        "    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n",
        "    one_hot_encoding[data_point_index] = 1\n",
        "    return one_hot_encoding"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JigWvP5NPHEP"
      },
      "source": [
        "X = [] # input word\n",
        "Y = [] # target word\n",
        "\n",
        "for x, y in zip(df['input'], df['label']):\n",
        "    X.append(to_one_hot_encoding(word2int[ x ]))\n",
        "    Y.append(to_one_hot_encoding(word2int[ y ]))\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se6Xcl_HPWNW",
        "outputId": "a59e0256-48f8-4280-fdcd-a3d6d0158d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "df.iloc[0], X[0], Y[0], word2int[df.input[0]], word2int[df.label[0]]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(input      king\n",
              " label    strong\n",
              " Name: 0, dtype: object,\n",
              " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
              " 1,\n",
              " 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwPEnYGgPwAM"
      },
      "source": [
        "Here we can see, index in word2int is 1 for the corresponding word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qtgt_MpTP6Rw"
      },
      "source": [
        "# convert them to numpy arrays\n",
        "X_train = np.asarray(X)\n",
        "Y_train = np.asarray(Y)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vxy19agHbBs"
      },
      "source": [
        "# Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxN0Thxir-6v"
      },
      "source": [
        "# word embedding will be 2 dimension for 2d visualization\n",
        "EMBEDDING_DIM = 2  #2 neurons in hidden layer"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3scC_b8sHefs"
      },
      "source": [
        "#cross entropy loss function\n",
        "@tf.function\n",
        "def custom_loss(y_true, y_pred):\n",
        "  loss =  tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(tf.clip_by_value(y_pred,1e-12,1.0)), axis=[1]))\n",
        "  #loss =  tf.nn.softmax_cross_entropy_with_logits(labels=y_pred,logits=y_true)\n",
        "  return loss"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA2hbSUeHk4E"
      },
      "source": [
        "#RMSProp optimizer as we're dealing with sparse matrix\n",
        "rms_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ABq03WN79O5"
      },
      "source": [
        "# Implementation with TensorFlow Custom Layers\n",
        "\n",
        "<a href = \"https://stackoverflow.com/questions/57086301/custom-neural-network-implementation-on-mnist-using-tensorflow-2-0\">Model Preparation with the help of this link</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7FE8P6P4z6s"
      },
      "source": [
        "class HiddenLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "\n",
        "    # Use build to create variables, as shape can be inferred from previous layers\n",
        "    # If you were to create layers in __init__, one would have to provide input_shape\n",
        "    # (same as it occurs in PyTorch for example)\n",
        "    def build(self, input_shape):\n",
        "        # You could use different initializers here as well\n",
        "        self.kernel = self.add_weight(name = 'W1',\n",
        "            shape=(ONE_HOT_DIM, EMBEDDING_DIM),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.bias = self.add_weight(shape=(self.units,), initializer=\"random_normal\")\n",
        "        # Oh, trainable=True is default\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Use overloaded operators instead of tf.add, better readability\n",
        "        return tf.matmul(inputs, self.kernel) + self.bias\n",
        "\t\t\n",
        "\t\t\n",
        "class OutputLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.bias = self.add_weight(shape=(self.units,), initializer=\"random_normal\")\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        return tf.nn.softmax(tf.matmul(inputs, self.kernel) + self.bias)\n",
        "\t\t\n",
        "class MyWord2VecTF2(tf.keras.Model):\n",
        "    def __init__(self, hidden_units):\n",
        "        super().__init__()\n",
        "        # Use Sequential here for readability\n",
        "        self.network = tf.keras.Sequential(\n",
        "            [HiddenLayer(hidden_units), OutputLayer(ONE_HOT_DIM)]\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # You can use non-parametric layers inside call as well\n",
        "        flattened = tf.keras.layers.Flatten()(inputs)\n",
        "        return self.network(flattened)"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgbE6ctfuk8U"
      },
      "source": [
        "<a href = \"https://github.com/minsuk-heo/python_tutorial/blob/master/data_science/nlp/word2vec_tensorflow.ipynb\"> Model has been prepared using this notebook</a>\n",
        "\n",
        "<a href = \"https://stackoverflow.com/questions/44747343/keras-input-explanation-input-shape-units-batch-size-dim-etc\">This explains the above arguments to tf</a>\n",
        "\n",
        "<a href = \"https://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable\">Placeholder vs Variables</a>\n",
        "\n",
        "<a href = \"https://medium.com/red-buffer/tensorflow-1-0-to-tensorflow-2-0-coding-changes-636b49a604b\" > Also used this for basic migration from tf 1 to tf 2</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP9cdkng9PCI"
      },
      "source": [
        "# Implementation with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUj82VgQZYM4"
      },
      "source": [
        "class MyWord2VecKeras(tf.keras.Model):\n",
        "  def __init__(self, hidden_units):\n",
        "    super(MyWord2Vec, self).__init__()\n",
        "    self.dense_output = tf.keras.layers.Dense(units=ONE_HOT_DIM, activation=tf.nn.softmax)\n",
        "    self.dense_hidden = tf.keras.layers.Dense(units=hidden_units)\n",
        "    self.input_layer = tf.keras.layers.Flatten(input_shape = (None, ONE_HOT_DIM))\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = self.input_layer(x)\n",
        "    x = self.dense_hidden(x)\n",
        "    x = self.dense_output(x)\n",
        "    return x"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii-y22L0J0t-"
      },
      "source": [
        "# Model Instantiation and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkb74ltLKPc7"
      },
      "source": [
        "class MyModel:\n",
        "  def __init__(self, hidden_units, model_type = 'tf2'):\n",
        "    self.model = None\n",
        "    if model_type == 'tf2':\n",
        "      self.model = MyWord2VecTF2(EMBEDDING_DIM) #core tf 2 implementation\n",
        "    else:\n",
        "      self.model = MyWord2VecKeras(EMBEDDING_DIM) #for keras Model"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxhrASAtnljX"
      },
      "source": [
        "model = MyModel(EMBEDDING_DIM, 'tf2').model"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcgT-CsC42Y9"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=rms_optimizer,\n",
        "    loss=custom_loss\n",
        ")"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WspmV5Cn6dvy",
        "outputId": "5b2c4f0d-5e5c-4b1c-eecb-f33cf31de28c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x_train, y_train, epochs=200)"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.4896\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 2.4740\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.4596\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.4498\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.4379\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 2.4276\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.4192\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.4055\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.3975\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 2.3857\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.3775\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 2.3668\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 2.3589\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 2.3490\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 2.3385\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 2.3310\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 2.3263\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 2.3143\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 2.3084\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.3017\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 2.2947\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 2.2857\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.2770\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.2704\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.2633\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.2558\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.2482\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.2398\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.2329\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.2276\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 2.2200\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.2116\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.2057\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1969\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1890\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1826\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1786\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1689\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1621\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1533\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1473\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1423\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1351\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1278\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1232\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1170\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1083\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1046\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.1013\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0919\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0833\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0777\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0697\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0653\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0593\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0580\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 2.0476\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0429\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0358\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0337\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0276\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 2.0197\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0158\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0096\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0078\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9993\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 2.0012\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9892\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9851\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9827\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.9771\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9713\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9682\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9661\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9594\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.9558\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.9499\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.9479\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.9430\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.9415\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9356\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.9314\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9323\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.9259\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9216\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9152\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9139\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9100\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.9057\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.9007\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8981\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8994\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8947\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8922\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8872\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8863\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.8811\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.8791\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.8790\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8742\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.8718\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8711\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8678\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8640\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8697\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8627\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8577\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8592\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8567\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8528\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 1.8509\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8488\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8485\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8434\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8418\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8429\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.8374\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8371\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.8373\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.8296\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.8346\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.8307\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.8302\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8255\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8243\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 1.8218\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8219\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8223\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.8200\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8183\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 1.8209\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8154\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8119\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8150\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8114\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8108\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8077\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8056\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8059\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8048\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8016\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8001\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8022\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7982\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.8002\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7950\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7927\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7922\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7941\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7923\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7879\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7909\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7844\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7871\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7827\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7839\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7827\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7814\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7793\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7770\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7808\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7747\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7799\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7751\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7764\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7748\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7705\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7696\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7683\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7673\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7684\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7674\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7633\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7645\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7611\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7620\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7619\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7605\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.7607\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7580\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7628\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7537\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7537\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7535\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7575\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7500\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7504\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7520\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 1.7479\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7479\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7477\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7461\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7532\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7477\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7476\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7404\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7437\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7451\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7418\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 1.7398\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f890d18ee10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w89db33GK-z9"
      },
      "source": [
        "# Getting the hidden layer parameters\n",
        "\n",
        "Since our fake task of generating context is done, now time to get word vectors from hidden layer parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4wG8Gg6_588",
        "outputId": "2ec454e2-51ed-459a-b914-6e87c12c5502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "b1 = model.network.get_layer(index=0).bias.numpy()\n",
        "W1 = model.network.get_layer(index=0).kernel.numpy()\n",
        "vectors = W1 + b1\n",
        "print(vectors)"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2.5188022   0.93887347]\n",
            " [ 1.3514935   0.52499413]\n",
            " [ 0.11886011 -1.0395901 ]\n",
            " [-0.50528145 -3.2685986 ]\n",
            " [ 1.6563097  -0.84091216]\n",
            " [ 2.2059987  -2.8390076 ]\n",
            " [-1.1173006   0.49354368]\n",
            " [-0.20038411 -2.0559227 ]\n",
            " [-1.010278   -0.57078254]\n",
            " [-0.49949157 -0.8007641 ]\n",
            " [-1.5529225  -3.0738468 ]\n",
            " [ 2.8800201  -0.4207744 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbYZwL2gKRZg",
        "outputId": "bc482cec-46fd-46b9-9c4d-5701f09b7787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\n",
        "w2v_df['word'] = words\n",
        "w2v_df = w2v_df[['word', 'x1', 'x2']]\n",
        "w2v_df"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>man</td>\n",
              "      <td>2.518802</td>\n",
              "      <td>0.938873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>king</td>\n",
              "      <td>1.351493</td>\n",
              "      <td>0.524994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>young</td>\n",
              "      <td>0.118860</td>\n",
              "      <td>-1.039590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pretty</td>\n",
              "      <td>-0.505281</td>\n",
              "      <td>-3.268599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>boy</td>\n",
              "      <td>1.656310</td>\n",
              "      <td>-0.840912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>strong</td>\n",
              "      <td>2.205999</td>\n",
              "      <td>-2.839008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>woman</td>\n",
              "      <td>-1.117301</td>\n",
              "      <td>0.493544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>girl</td>\n",
              "      <td>-0.200384</td>\n",
              "      <td>-2.055923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>princess</td>\n",
              "      <td>-1.010278</td>\n",
              "      <td>-0.570783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>queen</td>\n",
              "      <td>-0.499492</td>\n",
              "      <td>-0.800764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>wise</td>\n",
              "      <td>-1.552922</td>\n",
              "      <td>-3.073847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>prince</td>\n",
              "      <td>2.880020</td>\n",
              "      <td>-0.420774</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        word        x1        x2\n",
              "0        man  2.518802  0.938873\n",
              "1       king  1.351493  0.524994\n",
              "2      young  0.118860 -1.039590\n",
              "3     pretty -0.505281 -3.268599\n",
              "4        boy  1.656310 -0.840912\n",
              "5     strong  2.205999 -2.839008\n",
              "6      woman -1.117301  0.493544\n",
              "7       girl -0.200384 -2.055923\n",
              "8   princess -1.010278 -0.570783\n",
              "9      queen -0.499492 -0.800764\n",
              "10      wise -1.552922 -3.073847\n",
              "11    prince  2.880020 -0.420774"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jxDgFmQLUfu"
      },
      "source": [
        "# Finally, the Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrbKofEKKXWK",
        "outputId": "e093b9b7-6b2e-46b5-f591-14eb87cd29a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n",
        "    ax.annotate(word, (x1,x2 ))\n",
        "    \n",
        "PADDING = 1.0\n",
        "x_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n",
        "y_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n",
        "x_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n",
        "y_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n",
        " \n",
        "plt.xlim(x_axis_min,x_axis_max)\n",
        "plt.ylim(y_axis_min,y_axis_max)\n",
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}